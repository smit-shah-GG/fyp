{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wV_mldFOpq-",
        "outputId": "e4f7151e-df37-479d-b403-b7e487104f9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded keywords: [('default', 1, 0, 0), ('import', 1, 0, 0), ('CNN', 0, 1, 1), ('RNN', 1, 0, 1), ('preprocess', 1, 1, 0), ('train', 0, 1, 1), ('eval', 1, 0, 0), ('transfer', 0, 1, 1), ('feature', 1, 1, 0), ('tune', 1, 0, 0), ('grid', 1, 0, 0), ('LSTM', 1, 0, 1), ('data_augmentation', 1, 1, 0), ('batch_normalization', 0, 1, 1), ('dropout', 1, 0, 0), ('optimization', 1, 1, 1), ('model_saving', 1, 0, 0), ('hyperparameter_search', 1, 1, 0), ('data_loading', 1, 1, 0), ('evaluation_metrics', 1, 0, 0), ('fine_tuning', 0, 1, 1), ('ensemble_learning', 1, 1, 0), ('tokenization', 1, 0, 0), ('backpropagation', 1, 0, 0), ('regularization', 1, 0, 0), ('transfer_learning', 0, 1, 1), ('seq2seq', 1, 0, 1), ('attention_mechanism', 0, 1, 1), ('model_export', 1, 0, 0), ('real_time_inference', 0, 1, 1), ('predictive_analysis', 1, 0, 0), ('class_weighting', 1, 1, 0), ('grid_search_cv', 1, 0, 0), ('cross_validation', 1, 1, 0), ('feature_scaling', 1, 1, 0), ('loss_function', 1, 0, 0), ('convolutional_layer', 0, 1, 1), ('pooling_layer', 0, 1, 0), ('output_layer', 1, 0, 0), ('activation_function', 1, 0, 0), ('parameter_tuning', 1, 1, 1), ('data_split', 1, 0, 0), ('model_validation', 1, 1, 0), ('neural_network', 1, 0, 1), ('autoencoder', 1, 0, 1), ('multilayer_perceptron', 1, 0, 0), ('k_fold_cv', 1, 1, 0), ('pipeline', 1, 1, 0), ('data_normalization', 1, 1, 0), ('sampling', 1, 1, 0), ('label_encoding', 1, 0, 0), ('one_hot_encoding', 1, 1, 0), ('dimensionality_reduction', 1, 1, 0), ('feature_selection', 1, 0, 0), ('model_ensemble', 1, 1, 0), ('gradient_descent', 1, 0, 0), ('stochastic_gradient_descent', 1, 0, 0), ('momentum', 1, 0, 0), ('learning_rate_schedule', 1, 1, 0), ('early_stopping', 1, 0, 0), ('loss_backpropagation', 1, 0, 0), ('training_epochs', 1, 0, 0), ('predict', 1, 0, 0), ('confusion_matrix', 1, 0, 0), ('roc_curve', 1, 0, 0), ('precision_recall', 1, 0, 0), ('hyperparameter_tuning', 1, 1, 0), ('cross_entropy', 1, 0, 0), ('binary_crossentropy', 1, 0, 0), ('categorical_crossentropy', 1, 0, 0), ('sparse_categorical_crossentropy', 1, 0, 0), ('focal_loss', 1, 0, 0), ('adam_optimizer', 1, 1, 1), ('sgd_optimizer', 1, 0, 0), ('rmsprop_optimizer', 1, 1, 1), ('learning_rate', 1, 1, 0), ('data_generator', 1, 1, 0), ('image_processing', 1, 1, 0), ('text_processing', 1, 0, 0), ('time_series_analysis', 1, 0, 0), ('feature_importance', 1, 1, 0), ('xgboost', 1, 0, 0), ('lightgbm', 1, 0, 0), ('catboost', 1, 0, 0), ('model_inference', 1, 0, 0), ('decision_tree', 1, 0, 0), ('random_forest', 1, 0, 0), ('svm', 1, 0, 0), ('clustering', 1, 0, 0), ('anomaly_detection', 1, 0, 0), ('reinforcement_learning', 1, 0, 0), ('policy_gradient', 1, 0, 0), ('q_learning', 1, 0, 0), ('deep_q_network', 1, 0, 1), ('image_classification', 1, 0, 1), ('object_detection', 1, 1, 1), ('semantic_segmentation', 1, 1, 1), ('instance_segmentation', 1, 1, 1), ('natural_language_processing', 1, 0, 0), ('bert', 1, 1, 1), ('gpt', 1, 1, 1), ('transformers', 1, 1, 1), ('word_embedding', 1, 1, 0), ('feature_extraction', 1, 1, 0), ('vgg', 1, 1, 1), ('resnet', 1, 1, 1), ('inception', 1, 1, 1), ('mobilenet', 1, 1, 1), ('efficientnet', 1, 1, 1), ('auto_ml', 1, 1, 0), ('k_means', 1, 0, 0), ('pca', 1, 1, 0), ('data_visualization', 1, 0, 0), ('matplotlib', 1, 0, 0), ('seaborn', 1, 0, 0), ('plotly', 1, 0, 0), ('dashboard', 1, 0, 0), ('streamlit', 1, 0, 0), ('flask', 1, 0, 0), ('django', 1, 0, 0), ('api_deployment', 1, 0, 0), ('containerization', 1, 1, 0), ('model_serving', 1, 0, 0), ('data_pipeline', 1, 0, 0), ('cloud_computing', 1, 1, 0), ('data_security', 1, 0, 0), ('data_privacy', 1, 0, 0), ('compliance', 1, 0, 0), ('data_labeling', 1, 0, 0), ('data_cataloging', 1, 0, 0), ('etl_process', 1, 1, 0), ('data_quality', 1, 0, 0), ('data_modeling', 1, 0, 0), ('business_intelligence', 1, 0, 0), ('data_science', 1, 0, 0), ('data_engineering', 1, 0, 0), ('data_analysis', 1, 0, 0), ('outlier_detection', 1, 0, 0), ('neural_architecture_search', 1, 1, 0), ('federated_learning', 1, 1, 0), ('ensemble_methods', 1, 0, 0), ('semi_supervised_learning', 1, 0, 0), ('active_learning', 1, 0, 0), ('self_supervised_learning', 1, 0, 0), ('multi_task_learning', 1, 0, 0), ('batch_size', 1, 0, 0), ('gradient_boosting', 1, 0, 0), ('model_pruning', 1, 1, 0), ('neural_style_transfer', 1, 0, 0), ('hyperparameter_optimization', 1, 1, 0), ('knowledge_distillation', 1, 0, 0), ('model_compression', 1, 0, 0), ('data_shuffling', 1, 0, 0), ('input_pipeline', 1, 1, 0), ('memory_management', 1, 0, 0), ('multi_class_classification', 1, 0, 0), ('binary_classification', 1, 0, 0), ('data_mining', 1, 0, 0), ('text_classification', 1, 0, 0), ('sentiment_analysis', 1, 0, 0), ('named_entity_recognition', 1, 0, 0), ('text_generation', 1, 0, 0), ('text_summarization', 1, 0, 0), ('speech_recognition', 1, 0, 0), ('chatbot', 1, 0, 0), ('voice_assistant', 1, 0, 0), ('graph_neural_network', 1, 1, 1), ('reinforcement_learning_algorithm', 1, 0, 0), ('contextual_bandits', 1, 0, 0), ('transferability', 1, 0, 0), ('evaluation_protocols', 1, 0, 0), ('model_interpretability', 1, 1, 0), ('LIME', 1, 0, 0), ('SHAP', 1, 1, 0), ('feature_map', 1, 0, 0), ('residual_connections', 1, 1, 0), ('attention_layer', 1, 1, 0), ('transformer_encoder', 1, 1, 1), ('transformer_decoder', 1, 1, 1), ('multi_head_attention', 1, 1, 1), ('positional_encoding', 1, 0, 0), ('sequence_padding', 1, 0, 0), ('sampling_strategy', 1, 0, 0), ('class_balance', 1, 0, 0), ('feature_engineering', 1, 0, 0), ('ensemble_selection', 1, 0, 0), ('data_enrichment', 1, 0, 0), ('data_integrity', 1, 0, 0), ('data_profiling', 1, 0, 0), ('data_warehouse', 1, 0, 0), ('data_lake', 1, 0, 0), ('cloud_storage', 1, 1, 0), ('batch_processing', 1, 0, 0), ('stream_processing', 1, 1, 0), ('real_time_streaming', 1, 1, 0), ('data_fusion', 1, 0, 0), ('computational_graph', 1, 1, 0), ('graph_theory', 1, 0, 0), ('maml', 1, 0, 0), ('meta_learning', 1, 0, 0), ('few_shot_learning', 1, 0, 0), ('zero_shot_learning', 1, 0, 0), ('data_representation', 1, 0, 0), ('data_synthesis', 1, 0, 0), ('data_preparation', 1, 0, 0), ('data_validation', 1, 0, 0), ('performance_benchmarking', 1, 0, 0), ('model_selection', 1, 1, 0), ('model_retraining', 1, 0, 0), ('data_labeling_tool', 1, 0, 0), ('data_scientist', 1, 0, 0), ('data_engineer', 1, 0, 0), ('machine_learning_engineer', 1, 0, 0), ('model_versioning', 1, 0, 0), ('hyperparameter_tuning_tool', 1, 1, 0), ('experiment_tracking', 1, 0, 0), ('software_development_lifecycle', 1, 0, 0), ('version_control', 1, 0, 0), ('devops', 1, 0, 0), ('mlops', 1, 1, 0), ('pipeline_automation', 1, 1, 0), ('feedback_loop', 1, 0, 0), ('model_feedback', 1, 0, 0), ('data_security_compliance', 1, 0, 0), ('data_protection', 1, 0, 0), ('algorithmic_bias', 1, 0, 0), ('model_ethics', 1, 0, 0), ('explainable_ai', 1, 1, 0), ('transparent_ml', 1, 0, 0), ('cloud_infrastructure', 1, 1, 0), ('big_data', 1, 1, 0), ('data_scalability', 1, 1, 0), ('image_segmentation', 1, 0, 0), ('semantic_segmentation', 1, 1, 1), ('instance_segmentation', 1, 1, 1), ('image_classification', 1, 0, 0), ('object_detection', 1, 1, 0), ('style_transfer', 1, 0, 0), ('data_augmentation_techniques', 1, 0, 0), ('text_embedding', 1, 0, 0), ('feature_selection', 1, 0, 0), ('time_series_analysis', 1, 0, 0), ('anomaly_detection', 1, 0, 0), ('dimensionality_reduction', 1, 0, 0), ('pca', 1, 0, 0), ('t_sne', 1, 0, 0), ('vae', 1, 0, 0), ('autoencoder', 1, 0, 0), ('deep_fakes', 1, 0, 0), ('data_cleansing', 1, 0, 0), ('data_normalization', 1, 0, 0), ('categorical_encoding', 1, 0, 0), ('target_encoding', 1, 0, 0), ('label_encoding', 1, 0, 0), ('word_embedding', 1, 0, 0), ('fasttext', 1, 0, 0), ('word2vec', 1, 0, 0), ('glove', 1, 0, 0), ('contextual_word_embeddings', 1, 0, 0), ('transformer_model', 1, 1, 1), ('bert', 1, 1, 1), ('gpt', 1, 1, 1), ('xlmr', 1, 1, 1), ('ner_model', 1, 1, 0), ('ml_model_deployment', 1, 0, 0), ('api_integration', 1, 0, 0), ('cloud_ml_services', 1, 1, 0), ('distributed_training', 1, 1, 1), ('hyperparameter_sweeping', 1, 1, 0), ('data_slicing', 1, 0, 0), ('data_summarization', 1, 0, 0), ('text_vectorization', 1, 0, 0), ('sparse_data_handling', 1, 0, 0), ('model_ensembling', 1, 0, 0), ('feature_importance', 1, 0, 0), ('gradient_descent', 1, 0, 0), ('stochastic_gradient_descent', 1, 0, 0), ('adam_optimizer', 1, 0, 0), ('momentum_optimizer', 1, 0, 0), ('learning_rate_scheduler', 1, 0, 0), ('regularization_techniques', 1, 0, 0), ('dropout_rate', 1, 0, 0), ('early_stopping', 1, 0, 0), ('learning_rate_finder', 1, 0, 0), ('cross_entropy_loss', 1, 0, 0), ('mean_squared_error', 1, 0, 0), ('binary_crossentropy', 1, 0, 0), ('multi_class_crossentropy', 1, 0, 0), ('f1_score', 1, 0, 0), ('precision_recall_curve', 1, 0, 0), ('roc_auc', 1, 0, 0), ('accuracy_score', 1, 0, 0), ('training_curve', 1, 0, 0), ('overfitting_detection', 1, 0, 0), ('data_partitioning', 1, 0, 0), ('train_test_split', 1, 0, 0), ('holdout_validation', 1, 0, 0), ('k_fold_cross_validation', 1, 0, 0), ('stratified_sampling', 1, 0, 0), ('pipeline_management', 1, 0, 0), ('model_registry', 1, 0, 0), ('data_access_management', 1, 0, 0), ('visualization_tools', 1, 0, 0), ('data_storytelling', 1, 0, 0), ('interactive_visualization', 1, 0, 0), ('dashboard_creation', 1, 0, 0), ('real_time_dashboard', 1, 0, 0), ('reporting_tools', 1, 0, 0), ('automated_reporting', 1, 0, 0), ('business_intelligence', 1, 0, 0), ('model_update_strategy', 1, 0, 0), ('data_collection_strategy', 1, 0, 0), ('data_pipeline', 1, 0, 0), ('scraping_tools', 1, 0, 0), ('data_migration', 1, 0, 0), ('data_sharing_protocol', 1, 0, 0), ('compliance_standards', 1, 0, 0), ('data_retention_policy', 1, 0, 0), ('model_auditing', 1, 0, 0), ('feedback_analysis', 1, 0, 0)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPU initialization failed: Please provide a TPU Name to connect to.\n",
            "TPU is required but unavailable. Proceeding with fallback to GPU or CPU.\n",
            "Executing snippet on /device:GPU:0\n",
            "Executing snippet on /device:GPU:0\n",
            "Executing snippet on /device:GPU:0\n",
            "Error executing snippet: You must call `compile()` before using the model.\n",
            "Snippet action create_model -> <Sequential name=sequential, built=True>\n",
            "Epoch 1/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.3581 - loss: 1.7411\n",
            "Epoch 2/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.5843 - loss: 1.1705\n",
            "Epoch 3/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.6481 - loss: 0.9899\n",
            "Epoch 4/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.6955 - loss: 0.8696\n",
            "Epoch 5/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7250 - loss: 0.7839\n",
            "Epoch 6/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7450 - loss: 0.7230\n",
            "Epoch 7/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7579 - loss: 0.6870\n",
            "Epoch 8/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7815 - loss: 0.6257\n",
            "Epoch 9/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7877 - loss: 0.6028\n",
            "Epoch 10/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8086 - loss: 0.5498\n",
            "Snippet action train_model -> Execution completed in 51.37 seconds on /device:GPU:0.\n",
            "Total execution time for code: 52.53 seconds\n"
          ]
        }
      ],
      "source": [
        "import concurrent.futures\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "# Initialize TPU if needed\n",
        "def init_tpu():\n",
        "    try:\n",
        "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver()  # Detect TPU\n",
        "        tf.config.experimental_connect_to_cluster(resolver)\n",
        "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "        print(\"TPU initialized.\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"TPU initialization failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# Load keywords and resource needs from Excel file\n",
        "def load_keywords_from_excel(file_path):\n",
        "    try:\n",
        "        df = pd.read_excel(file_path)\n",
        "        df.columns = df.columns.str.strip()  # Strip whitespace from column names\n",
        "        keywords = [(row['Keyword'], int(row['CPU']), int(row['GPU']), int(row['TPU'])) for _, row in df.iterrows()]\n",
        "        print(f\"Loaded keywords: {keywords}\")\n",
        "        return keywords\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "# Define CNN model\n",
        "def create_cnn_model(input_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile and train CNN model\n",
        "def compile_and_train_model(model, train_images, train_labels, epochs=10):\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(train_images, train_labels, epochs=epochs)\n",
        "\n",
        "# Evaluate CNN model\n",
        "def evaluate_model(model, test_images, test_labels):\n",
        "    test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
        "    print(f'\\nTest accuracy: {test_acc}')\n",
        "    return test_acc\n",
        "\n",
        "# Load and preprocess CIFAR-10 dataset\n",
        "def load_and_preprocess_data():\n",
        "    (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "    train_images = train_images.astype('float32') / 255.0\n",
        "    test_images = test_images.astype('float32') / 255.0\n",
        "    return train_images, train_labels, test_images, test_labels\n",
        "\n",
        "# Execute a CNN snippet on the specified device and measure execution time\n",
        "def run_snippet(snippet, device):\n",
        "    print(f\"Executing snippet on {device}\")  # Print device info\n",
        "    start_time = time.time()\n",
        "    accuracy = None\n",
        "\n",
        "    with tf.device(device):\n",
        "        if snippet[\"action\"] == \"create_model\":\n",
        "            return create_cnn_model((32, 32, 3))\n",
        "        elif snippet[\"action\"] == \"train_model\":\n",
        "            compile_and_train_model(snippet[\"model\"], snippet[\"train_images\"], snippet[\"train_labels\"])\n",
        "        elif snippet[\"action\"] == \"evaluate_model\":\n",
        "            accuracy = evaluate_model(snippet[\"model\"], snippet[\"test_images\"], snippet[\"test_labels\"])\n",
        "\n",
        "    execution_time = time.time() - start_time\n",
        "    return f\"Execution completed in {execution_time:.2f} seconds on {device}. Accuracy: {accuracy}\" if accuracy else f\"Execution completed in {execution_time:.2f} seconds on {device}.\"\n",
        "\n",
        "# Decide the device based on resource availability\n",
        "def get_device(cpu, gpu, tpu):\n",
        "    if tpu and tf.config.list_logical_devices('TPU'):\n",
        "        return '/device:TPU:0'\n",
        "    elif gpu and tf.config.list_physical_devices('GPU'):\n",
        "        return '/device:GPU:0'\n",
        "    else:\n",
        "        return '/device:CPU:0'\n",
        "\n",
        "# Main master function for scheduling tasks\n",
        "def master(file_path):\n",
        "    start_time = time.time()\n",
        "    keywords = load_keywords_from_excel(file_path)\n",
        "    if keywords is None:\n",
        "        print(\"Failed to load keywords.\")\n",
        "        return\n",
        "\n",
        "    # Load and preprocess data for training and evaluation\n",
        "    train_images, train_labels, test_images, test_labels = load_and_preprocess_data()\n",
        "\n",
        "    # Define CNN snippets with actions\n",
        "    cnn_snippets = [\n",
        "        {\"action\": \"create_model\"},\n",
        "        {\"action\": \"train_model\", \"model\": create_cnn_model((32, 32, 3)), \"train_images\": train_images, \"train_labels\": train_labels},\n",
        "        {\"action\": \"evaluate_model\", \"model\": create_cnn_model((32, 32, 3)), \"test_images\": test_images, \"test_labels\": test_labels}\n",
        "    ]\n",
        "\n",
        "    tpu_required = any(resource[-1] for resource in keywords)\n",
        "    if tpu_required:\n",
        "        tpu_available = init_tpu()\n",
        "        if not tpu_available:\n",
        "            print(\"TPU is required but unavailable. Proceeding with fallback to GPU or CPU.\")\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
        "        futures = {}\n",
        "        for snippet in cnn_snippets:\n",
        "            resources = (1, 1, 1)  # Example CPU, GPU, TPU allocations\n",
        "            device = get_device(*resources)\n",
        "            futures[executor.submit(run_snippet, snippet, device)] = snippet\n",
        "\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            snippet = futures[future]\n",
        "            try:\n",
        "                result = future.result()\n",
        "                print(f\"Snippet action {snippet['action']} -> {result}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error executing snippet: {e}\")\n",
        "\n",
        "    total_execution_time = time.time() - start_time\n",
        "    print(f\"Total execution time for code: {total_execution_time:.2f} seconds\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == '__main__':\n",
        "    excel_file_path = 'resource_allocation_dataset.xlsx'  # Update with the correct path\n",
        "    master(excel_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "def create_cnn_model(input_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def compile_and_train_model(model, train_images, train_labels, epochs=10):\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(train_images, train_labels, epochs=epochs)\n",
        "\n",
        "def evaluate_model(model, test_images, test_labels):\n",
        "    test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
        "    print(f'\\nTest accuracy: {test_acc}')\n",
        "\n",
        "def load_and_preprocess_data():\n",
        "    (train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "    train_images = train_images.astype('float32') / 255.0\n",
        "    test_images = test_images.astype('float32') / 255.0\n",
        "    return train_images, train_labels, test_images, test_labels\n",
        "\n",
        "# Measure CNN execution time\n",
        "start_time = time.time()\n",
        "\n",
        "# Load data\n",
        "train_images, train_labels, test_images, test_labels = load_and_preprocess_data()\n",
        "\n",
        "# Create model\n",
        "model = create_cnn_model((32, 32, 3))\n",
        "\n",
        "# Compile and train model\n",
        "compile_and_train_model(model, train_images, train_labels, epochs=10)  # Adjust epochs as needed\n",
        "\n",
        "# Evaluate model\n",
        "evaluate_model(model, test_images, test_labels)\n",
        "\n",
        "# Print execution time\n",
        "cnn_execution_time = time.time() - start_time\n",
        "print(f\"Total execution time for CNN code: {cnn_execution_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlOxpqa5UedI",
        "outputId": "963d30cb-60ae-498c-ee6e-25d513def737"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.3368 - loss: 1.7774\n",
            "Epoch 2/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.5645 - loss: 1.2193\n",
            "Epoch 3/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6247 - loss: 1.0554\n",
            "Epoch 4/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.6705 - loss: 0.9353\n",
            "Epoch 5/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.6967 - loss: 0.8733\n",
            "Epoch 6/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7174 - loss: 0.8022\n",
            "Epoch 7/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7388 - loss: 0.7510\n",
            "Epoch 8/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7480 - loss: 0.7154\n",
            "Epoch 9/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7688 - loss: 0.6611\n",
            "Epoch 10/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7823 - loss: 0.6224\n",
            "313/313 - 1s - 4ms/step - accuracy: 0.7161 - loss: 0.8526\n",
            "\n",
            "Test accuracy: 0.7160999774932861\n",
            "Total execution time for CNN code: 57.77 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W2hKvevDUqNk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}