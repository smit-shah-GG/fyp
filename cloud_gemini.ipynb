{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvdpFioD_2nQ"
      },
      "source": [
        "# Using Gemini in AutoGen with Other LLMs\n",
        "\n",
        "## Installation\n",
        "\n",
        "Install AutoGen with Gemini features:\n",
        "\n",
        "```bash\n",
        "pip install autogen-agentchat[gemini]~=0.2\n",
        "```\n",
        "\n",
        "## Dependencies of This Notebook\n",
        "\n",
        "In this notebook, we will explore how to use Gemini in AutoGen alongside other tools. Install the necessary dependencies with the following command:\n",
        "\n",
        "```bash\n",
        "pip install autogen-agentchat[gemini,retrievechat,lmm]~=0.2\n",
        "```\n",
        "\n",
        "## Features\n",
        "\n",
        "There's no need to handle OpenAI or Google's GenAI packages separately; AutoGen manages all of these for you. You can easily create different agents with various backend LLMs using the assistant agent. All models and agents are readily accessible at your fingertips.\n",
        "\n",
        "\n",
        "## Main Distinctions\n",
        "\n",
        "- Currently, Gemini does not include a \"system_message\" field. However, you can incorporate this instruction into the first message of your interaction.\n",
        "- If no API key is specified for Gemini, then authentication will happen using the default google auth mechanism for Google Cloud. Service accounts are also supported, where the JSON key file has to be provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI2SZYOQ_2nT"
      },
      "source": [
        "Sample OAI_CONFIG_LIST\n",
        "\n",
        "```python\n",
        "[\n",
        "    {\n",
        "        \"model\": \"gpt-35-turbo\",\n",
        "        \"api_key\": \"your OpenAI Key goes here\",\n",
        "    },\n",
        "    {\n",
        "        \"model\": \"gpt-4-vision-preview\",\n",
        "        \"api_key\": \"your OpenAI Key goes here\",\n",
        "    },\n",
        "    {\n",
        "        \"model\": \"dalle\",\n",
        "        \"api_key\": \"your OpenAI Key goes here\",\n",
        "    },\n",
        "    {\n",
        "        \"model\": \"gemini-pro\",\n",
        "        \"api_key\": \"your Google's GenAI Key goes here\",\n",
        "        \"api_type\": \"google\"\n",
        "    },\n",
        "    {\n",
        "        \"model\": \"gemini-1.5-pro-001\",\n",
        "        \"api_type\": \"google\"\n",
        "    },\n",
        "    {\n",
        "        \"model\": \"gemini-1.5-pro\",\n",
        "        \"project_id\": \"your-awesome-google-cloud-project-id\",\n",
        "        \"location\": \"us-west1\",\n",
        "        \"google_application_credentials\": \"your-google-service-account-key.json\"\n",
        "    },\n",
        "    {\n",
        "        \"model\": \"gemini-pro-vision\",\n",
        "        \"api_key\": \"your Google's GenAI Key goes here\",\n",
        "        \"api_type\": \"google\"\n",
        "    }\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "21R7Erfo_2nT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/smit/miniconda3/envs/twelve/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union\n",
        "\n",
        "import chromadb\n",
        "from PIL import Image\n",
        "from termcolor import colored\n",
        "\n",
        "import autogen\n",
        "from autogen import Agent, AssistantAgent, ConversableAgent, UserProxyAgent\n",
        "from autogen.agentchat.contrib.img_utils import _to_pil, get_image_data\n",
        "from autogen.agentchat.contrib.multimodal_conversable_agent import MultimodalConversableAgent\n",
        "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
        "from autogen.code_utils import DEFAULT_MODEL, UNKNOWN, content_str, execute_code, extract_code, infer_lang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flaml[automl] in /home/smit/miniconda3/envs/twelve/lib/python3.12/site-packages (2.3.1)\n",
            "Requirement already satisfied: NumPy>=1.17 in /home/smit/miniconda3/envs/twelve/lib/python3.12/site-packages (from flaml[automl]) (1.26.4)\n",
            "Requirement already satisfied: lightgbm>=2.3.1 in /home/smit/miniconda3/envs/twelve/lib/python3.12/site-packages (from flaml[automl]) (4.5.0)\n",
            "Requirement already satisfied: xgboost<3.0.0,>=0.90 in /home/smit/miniconda3/envs/twelve/lib/python3.12/site-packages (from flaml[automl]) (2.1.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /home/smit/miniconda3/envs/twelve/lib/python3.12/site-packages (from flaml[automl]) (1.13.1)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /home/smit/miniconda3/envs/twelve/lib/python3.12/site-packages (from flaml[automl]) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /home/smit/miniconda3/envs/twelve/lib/python3.12/site-packages (from flaml[automl]) (1.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/smit/miniconda3/envs/twelve/lib/python3.12/site-packages (from pandas>=1.1.4->flaml[automl]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/smit/miniconda3/envs/twelve/lib/python3.12/site-packages (from pandas>=1.1.4->flaml[automl]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/smit/miniconda3/envs/twelve/lib/python3.12/site-packages (from pandas>=1.1.4->flaml[automl]) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /home/smit/miniconda3/envs/twelve/lib/python3.12/site-packages (from scikit-learn>=1.0.0->flaml[automl]) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/smit/miniconda3/envs/twelve/lib/python3.12/site-packages (from scikit-learn>=1.0.0->flaml[automl]) (3.5.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /home/smit/miniconda3/envs/twelve/lib/python3.12/site-packages (from xgboost<3.0.0,>=0.90->flaml[automl]) (2.20.5)\n",
            "Requirement already satisfied: six>=1.5 in /home/smit/miniconda3/envs/twelve/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.4->flaml[automl]) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install flaml[automl]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kxUxnXBr_2nU"
      },
      "outputs": [],
      "source": [
        "# config_list_4v = autogen.config_list_from_json(\n",
        "#     \"OAI_CONFIG_LIST\",\n",
        "#     filter_dict={\n",
        "#         \"model\": [\"gpt-4-vision-preview\"],\n",
        "#     },\n",
        "# )\n",
        "\n",
        "# config_list_gpt4 = autogen.config_list_from_json(\n",
        "#     \"OAI_CONFIG_LIST\",\n",
        "#     filter_dict={\n",
        "#         \"model\": [\"gpt-4\", \"gpt-4-0314\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
        "#     },\n",
        "# )\n",
        "\n",
        "config_list_gemini = autogen.config_list_from_json(\n",
        "    \"OAF_CONFIG_LIST.json\",\n",
        "    filter_dict={\n",
        "        \"model\": [\"gemini-pro\", \"gemini-1.5-pro\", \"gemini-1.5-pro-001\", \"gemini-1.5-flash\"],\n",
        "    },\n",
        ")\n",
        "\n",
        "config_list_gemini_vision = autogen.config_list_from_json(\n",
        "    \"OAF_CONFIG_LIST.json\",\n",
        "    filter_dict={\n",
        "        \"model\": [\"gemini-1.5-flash\"],\n",
        "    },\n",
        ")\n",
        "\n",
        "seed = 25  # for caching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXGgFCTk_2nU"
      },
      "source": [
        "## Gemini Assistant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLnS-lRp_2nV",
        "outputId": "c35e619a-0b36-4b52-f15a-a45b5e84d0bf",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "assistant = AssistantAgent(\n",
        "    \"assistant\", llm_config={\"config_list\": config_list_gemini, \"seed\": seed}, max_consecutive_auto_reply=3\n",
        ")\n",
        "\n",
        "user_proxy = UserProxyAgent(\n",
        "    \"user_proxy\",\n",
        "    code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False},\n",
        "    human_input_mode=\"NEVER\",\n",
        "    is_termination_msg=lambda x: content_str(x.get(\"content\")).find(\"TERMINATE\") >= 0,\n",
        ")\n",
        "\n",
        "result = user_proxy.initiate_chat(assistant, message=\"Sort the array with Bubble Sort: [4, 1, 5, 2, 3]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRMpyUne_2nV",
        "outputId": "a15ed21f-8589-43cc-b094-16c1df47130d"
      },
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEiFKTbN_2nV"
      },
      "source": [
        "## Agent Collaboration and Interactions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrwp2hoS_2nW",
        "outputId": "2341a0eb-200e-438b-a029-b41dea0232fb"
      },
      "outputs": [],
      "source": [
        "gpt = AssistantAgent(\n",
        "    \"Gemini_1\",\n",
        "    system_message=\"\"\"You should ask weird, tricky, and concise questions.\n",
        "Ask the next question based on the answer given to the previous one.\"\"\",\n",
        "    llm_config={\"config_list\": config_list_gemini, \"seed\": seed},\n",
        "    max_consecutive_auto_reply=3,\n",
        ")\n",
        "\n",
        "gemini = AssistantAgent(\n",
        "    \"Gemini_2\",\n",
        "    system_message=\"\"\"Always answer questions within one sentence. \"\"\",\n",
        "    #                      system_message=\"answer:\",\n",
        "    llm_config={\"config_list\": config_list_gemini, \"seed\": seed},\n",
        "    max_consecutive_auto_reply=4,\n",
        ")\n",
        "\n",
        "\n",
        "gpt.initiate_chat(gemini, message=\"Do Transformers purchase auto insurance or health insurance?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o790Mmim_2nW"
      },
      "source": [
        "Let's switch position. Now, Gemini is the question raiser.\n",
        "\n",
        "This time, Gemini could not follow the system instruction well or evolve questions, because the Gemini does not handle system messages similar to GPTs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqQc2DY7_2nW"
      },
      "source": [
        "## Gemini Multimodal\n",
        "\n",
        "You can create multimodal agent for Gemini the same way as the GPT-4V and LLaVA.\n",
        "\n",
        "\n",
        "Note that the Gemini-pro-vision does not support chat yet. So, we only use the last message in the prompt for multi-turn chat. The behavior might be strange compared to GPT-4V and LLaVA models.\n",
        "\n",
        "Here, we ask a question about\n",
        "![](https://github.com/microsoft/autogen/blob/main/website/static/img/chat_example.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "front_matter": {
      "description": "Using Gemini with AutoGen",
      "tags": [
        "gemini"
      ]
    },
    "kernelspec": {
      "display_name": "twelve",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
